{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# รายวิชา: ข้อมูลขนาดใหญ่ (Big Data) — Week 2: File Formats & Modern Storage\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/witsarutsarai12-Academic/128-356-Big-Data/blob/main/BigData_Week2_FileFormats_and_Storage.ipynb)\n",
        "\n",
        "**Learning outcomes:**\n",
        "- อธิบายความต่างระหว่าง File System vs Database และ Data Lake vs Lakehouse\n",
        "- แยกแยะ Row-format vs Columnar-format พร้อมเหตุผลเชิงประสิทธิภาพ (compression, column pruning, predicate pushdown)\n",
        "- แปลง CSV → Parquet และ benchmark ขนาด/เวลาอ่านเขียนได้\n",
        "- ใช้ DuckDB/Pandas อ่าน Parquet และออกแบบ mini-lab เพื่อสาธิตประสิทธิภาพ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ea4002",
      "metadata": {},
      "source": [
        "## Part 0: เตรียมสภาพแวดล้อม\n",
        "- รันบน Google Colab หรือเครื่องที่มี RAM ≥ 8GB\n",
        "- ไลบรารี: `pandas`, `numpy`, `pyarrow`/`fastparquet`, `duckdb`\n",
        "- ถ้าเพิ่งเปิด Colab ใหม่ ให้ติดตั้งด้านล่างก่อน\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958f9727",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ตรวจสอบเวอร์ชัน Python\n",
        "import sys\n",
        "sys.version\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baad2746",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ติดตั้งไลบรารีที่ต้องใช้ (รันครั้งเดียวพอ)\n",
        "%pip -q install pandas pyarrow fastparquet duckdb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a33d31c",
      "metadata": {},
      "source": [
        "## Part 1: The Computer & Data — Mental Model\n",
        "- **Storage (SSD/HDD)** = ตู้เย็น/ห้องเก็บของ — จุเยอะ, ช้า, ปิดไฟของยังอยู่\n",
        "- **RAM** = โต๊ะเขียง — พื้นที่น้อยแต่เร็ว ต้องยกของจากตู้เย็นมาก่อนถึงจะปรุงได้\n",
        "- **CPU** = พ่อครัว — ลงมือหั่น/ผัด\n",
        "- **กฎเหล็ก**: ทุกอย่างต้องถูกโหลดจาก Storage → RAM → CPU เสมอ\n",
        "\n",
        "![Kitchen Analogy](images/kitchen_analogy.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9bdd3d3",
      "metadata": {},
      "source": [
        "### Key Terms — Storage & Filesystems\n",
        "- **Block/Cluster**: หน่วยเล็กสุดที่ disk ใช้เก็บข้อมูล (เช่น 4KB)\n",
        "- **Inode/Metadata**: สมุดทะเบียนของไฟล์ (สิทธิ์, เวลา, ขนาด, ตำแหน่ง block)\n",
        "- **Page Cache**: RAM ที่ OS แอบเก็บไฟล์ที่เพิ่งอ่าน เพื่ออ่านซ้ำเร็วขึ้น\n",
        "- **Sequential vs Random I/O**: อ่านต่อเนื่องเร็วกว่าอ่านกระโดดเยอะ ๆ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a82c019",
      "metadata": {},
      "source": [
        "### File System Internals\n",
        "- ไฟล์ถูกหั่นเป็น **Block/Cluster** (ส่วนใหญ่ 4KB)\n",
        "- ไฟล์เล็กมากกินทั้งบล็อก → **Slack space**\n",
        "- ไฟล์ใหญ่ถูกกระจายหลายบล็อก → **Fragmentation**\n",
        "- เมื่อมีไฟล์เล็กจำนวนมาก (small files problem) ระบบต้องวิ่งหา block หลายตำแหน่ง → ช้า\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b783d92e",
      "metadata": {},
      "source": [
        "### File Read Path (เปิดไฟล์เกิดอะไรขึ้นบ้าง?)\n",
        "1) `open()` → OS หา metadata (inode)\n",
        "2) แปลง filename → block list\n",
        "3) OS ดึง block จาก disk → RAM (page cache)\n",
        "4) โปรแกรมอ่านจาก RAM (ไม่อ่านจาก disk ตรง ๆ)\n",
        "\n",
        "![File Read Path](images/file_read_path.png)\n",
        "\n",
        "**Key insight:** ถ้าอ่านซ้ำ และยังอยู่ใน page cache → เร็วมาก\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23c906a",
      "metadata": {},
      "source": [
        "### Fragmentation & Sequential Read\n",
        "- ถ้าไฟล์กระจายหลายตำแหน่ง → disk ต้อง “กระโดดหัวอ่าน” → ช้าลง\n",
        "- ระบบไฟล์สมัยใหม่พยายามจัดให้ไฟล์ใหญ่ต่อเนื่องกัน แต่ไฟล์เล็กจำนวนมากทำให้ fragmentation เพิ่ม\n",
        "\n",
        "![Disk Fragmentation](images/disk_fragmentation.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b9337a",
      "metadata": {},
      "source": [
        "### Distributed File Systems (HDFS / S3) — ภาพใหญ่ให้เห็น Block & Metadata\n",
        "- **HDFS**: ค่าเริ่มต้น block ~128MB พร้อม **replication factor = 3** เพื่อกันเครื่องล่ม; NameNode เก็บ metadata ไว้ใน RAM จึงเกลียดไฟล์จิ๋วจำนวนมาก\n",
        "- **การเขียนไฟล์**: client buffer ไว้จนเต็ม block แล้วค่อยส่งต่อ DataNode → ลด network round-trip\n",
        "- **Object Storage (S3/MinIO)**: ไม่มี concept block แต่ยังเจอปัญหาไฟล์เล็ก เพราะต้องเปิด request ทีละไฟล์และ metadata lookup หนัก\n",
        "- **ข้อแนะนำเชิงปฏิบัติ**: เล็งขนาดไฟล์ Parquet 128–512MB ต่อไฟล์ และจำนวน partition ให้พอดีกับเงื่อนไข query (เช่น วัน/เดือน ไม่ใช่ user_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b7c311",
      "metadata": {},
      "source": [
        "### HDFS vs Object Storage (S3) — ต่างกันยังไง\n",
        "| มิติ | HDFS | S3/Object Storage |\n",
        "|---|---|---|\n",
        "| หน่วยข้อมูล | block | object | \n",
        "| Metadata | NameNode (RAM-heavy) | Service metadata | \n",
        "| Latency | ต่ำกว่าใน cluster | สูงกว่าเล็กน้อย | \n",
        "| Update/Append | ทำได้ (append จำกัด) | ปกติ replace ทั้ง object | \n",
        "\n",
        "![HDFS vs S3](images/hdfs_vs_s3.png)\n",
        "\n",
        "**สรุป:** HDFS เหมาะกับ on-prem cluster; S3 เหมาะกับ cloud scale + durability\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39dd21d0",
      "metadata": {},
      "source": [
        "#### Small Files Problem: อาการ & วิธีแก้ (Deep Dive)\n",
        "\n",
        "**อาการ (Symptoms)**:\n",
        "- **NameNode / Metadata Service Overload**: สมมติเรามีไฟล์ 1TB ถ้าเก็บเป็น 1 ไฟล์ใหญ่ = 1 metadata entry. แต่ถ้าเก็บเป็น 1 ล้านไฟล์เล็ก (1MB/file) = 1 ล้าน entries ที่ต้องจำใน RAM ของ NameNode -> ระบบล่ม (OOM).\n",
        "- **Slow List/Read**: การ list file 1 ล้านไฟล์กินเวลานานมาก และการอ่านข้อมูลต้องเสียเวลา \"Open/Close\" ไฟล์บ่อยกว่าเวลาอ่านเนื้อหาจริง\n",
        "\n",
        "**Analogy (อุปมา)**:\n",
        "- เหมือนไปซื้อของ 1,000 ชิ้น: \n",
        "   - **Large File**: ได้ใบเสร็จยาว 1 ใบ (จัดการง่าย)\n",
        "   - **Small Files**: ได้ใบเสร็จสั้นๆ 1,000 ใบ (จัดการยาก, หายง่าย, ตรวจสอบช้า)\n",
        "\n",
        "![Small Files Bottleneck](images/small_files_bottleneck.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830ad08e",
      "metadata": {},
      "source": [
        "### Real-World Use Case: Log Data Platform (Shopee/Lazada style)\n",
        "**สถานการณ์**: มี User click logs เข้ามาวินาทีละ 100,000 events\n",
        "- **ถ้า Save ทุกวินาที**: จะได้ไฟล์เล็ก ๆ จำนวนมหาศาล (Small Files Problem) ภายในไม่กี่ชั่วโมง\n",
        "- **วิธีแก้**: ใช้ Buffer (Kafka/Memory) รอให้ครบ 5-10 นาที หรือครบ 500MB แล้วค่อยเขียนลง Storage เป็นไฟล์ใหญ่ 1 ไฟล์ (Batch Write)\n",
        "\n",
        "### Scenario Exercise — เลือกกลยุทธ์จัดไฟล์\n",
        "1) ข้อมูล log รายชั่วโมง 1MB ต่อไฟล์ ตลอดปี → จะ partition อย่างไร?  \n",
        "2) ข้อมูลธุรกรรม 200GB/วัน ต้อง query รายวัน → ตั้งขนาดไฟล์เป้าหมายเท่าไร?  \n",
        "3) ถ้าต้องอ่านแค่คอลัมน์ `status` กับ `date` เป็นหลัก → ควรเก็บแบบไหน?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48282000",
      "metadata": {},
      "source": [
        "### Activity 1 — Small Files Problem (ลงมือเอง)\n",
        "ให้ลองสร้างไฟล์เล็กจำนวนมาก แล้วดูเวลาที่ระบบใช้ในการ zip หรืออ่านรายชื่อไฟล์\n",
        "- คาดหวัง: การจัดการไฟล์ 5,000 ไฟล์ขนาด 1KB มักช้ากว่าไฟล์เดียวขนาด 5MB\n",
        "- ปรับจำนวนไฟล์แล้วสังเกตเวลา\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b730f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: สร้างไฟล์จิ๋วจำนวนมาก vs ไฟล์เดียว แล้ววัดเวลา listdir / zip\n",
        "import os, tempfile, time, shutil, pathlib\n",
        "\n",
        "N_FILES = 2000   # ปรับจำนวนไฟล์เล็กๆ\n",
        "SMALL_BYTES = 1024  # 1KB ต่อไฟล์\n",
        "\n",
        "base = pathlib.Path(tempfile.mkdtemp())\n",
        "small_dir = base / \"small\"\n",
        "big_dir = base / \"big\"\n",
        "small_dir.mkdir(); big_dir.mkdir()\n",
        "\n",
        "# สร้างไฟล์เล็กจำนวนมาก\n",
        "start = time.time()\n",
        "for i in range(N_FILES):\n",
        "    (small_dir / f\"f_{i}.txt\").write_bytes(b\"a\" * SMALL_BYTES)\n",
        "small_elapsed = time.time() - start\n",
        "\n",
        "# สร้างไฟล์ใหญ่ก้อนเดียวขนาดเท่ากัน\n",
        "big_path = big_dir / \"one_big.txt\"\n",
        "start = time.time()\n",
        "big_path.write_bytes(b\"a\" * SMALL_BYTES * N_FILES)\n",
        "big_elapsed = time.time() - start\n",
        "\n",
        "print(f\"สร้างไฟล์เล็ก {N_FILES} ไฟล์ ใช้เวลา {small_elapsed:.2f}s\")\n",
        "print(f\"สร้างไฟล์ใหญ่ไฟล์เดียว ใช้เวลา {big_elapsed:.2f}s\")\n",
        "\n",
        "# วัดเวลา listdir (metadata lookup)\n",
        "start = time.time(); len(list(small_dir.iterdir())); ls_small = time.time() - start\n",
        "start = time.time(); len(list(big_dir.iterdir())); ls_big = time.time() - start\n",
        "print(f\"listdir โฟลเดอร์ไฟล์เล็ก ใช้ {ls_small*1000:.1f} ms; โฟลเดอร์ไฟล์ใหญ่ ใช้ {ls_big*1000:.1f} ms\")\n",
        "\n",
        "shutil.rmtree(base)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df448491",
      "metadata": {},
      "source": [
        "### File System vs Database (ทำไมต้องมี DB?)\n",
        "| มิติ | File (CSV/TXT) | Database (MySQL/Postgres) |\n",
        "|---|---|---|\n",
        "| ความน่าเชื่อถือ | เขียนไม่จบไฟดับ = ไฟล์พัง | **ACID** คุมความถูกต้อง | \n",
        "| การค้นหา | ต้องอ่านทุกบรรทัด (scan) | มี **Index** กระโดดไปจุดที่ต้องการ |\n",
        "| การแก้ไข | แก้ 1 บรรทัด = เขียนใหม่ทั้งไฟล์ | แก้เฉพาะ row ได้ |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a65fc611",
      "metadata": {},
      "source": [
        "### CSV Reality Check — จุดที่มักพัง\n",
        "- **Delimiter/encoding ต่างกัน**: `,` vs `;` vs `\t`, UTF-8 vs Windows-874 → อ่านเพี้ยน\n",
        "- **Quoted fields**: ค่า string ที่มี comma / newline ต้องถูกล้อมด้วย `\"\"`; ถ้าไม่ใส่ → column shift\n",
        "- **Missing header / column drift**: ไฟล์จากคนละระบบอาจลำดับคอลัมน์ต่างกัน\n",
        "- **Type drift**: ตัวเลขที่มี leading zero (`00123`) ถูกแปลงเป็น int → ศูนย์หาย\n",
        "- แนวทาง: ระบุ `dtype` ชัด, ใช้ `quotechar`/`escapechar`, ตรวจ schema ก่อนใช้, ถ้าไฟล์ใหญ่ → พิจารณาแปลงเป็น Parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7835009",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ตัวอย่างปัญหา CSV: comma ภายในข้อความ\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "raw = \"id,name,comment\n",
        "1,Alice,\"Likes, commas\"\n",
        "2,Bob,Plain text\n",
        "\"\n",
        "print(pd.read_csv(StringIO(raw)))\n",
        "print('\n",
        "กำหนด quotechar ถูกต้อง:')\n",
        "print(pd.read_csv(StringIO(raw), quotechar='\"'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f5db812",
      "metadata": {},
      "source": [
        "### File vs DB — ใช้ตอนไหน? (Comparison Strategy)\n",
        "\n",
        "![File vs DB Decision](images/file_vs_db_decision.png)\n",
        "\n",
        "| Feature | File System (Parquet/CSV) | Database (MySQL/Postgres) |\n",
        "|---|---|---|\n",
        "| **Primary Goal** | Analytics / Batch Processing (อ่านเยอะ เขียนครั้งเดียว) | Transactional / Real-time Updates (อ่านเขียนตลอดเวลา) |\n",
        "| **Cost** | ถูก (S3/HDFS) | แพง (Server Resource/License) |\n",
        "| **Update** | ยาก (ต้องเขียนทับทั้งไฟล์) | ง่าย (Update ทีละ row ได้) |\n",
        "| **ACID** | ไม่มี (ต้องใช้ Lakehouse ช่วย) | มีครบ (รับประกันความถูกต้องสูงสุด) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "908c24a5",
      "metadata": {},
      "source": [
        "## Part 2: The Bottleneck — Stack vs Heap\n",
        "- **Stack**: ตัวเลข/ตัวแปรเล็ก + ตัวชี้ (reference) ไปยังข้อมูลจริง\n",
        "- **Heap**: เก็บวัตถุจริง (list, DataFrame) กินพื้นที่เยอะ\n",
        "- **Garbage Collector**: เก็บของใน Heap ที่ไม่มี reference ชี้ถึง\n",
        "- Analogy: รีโมท (stack) ↔ ทีวี (heap)\n",
        "\n",
        "![Stack vs Heap](images/stack_vs_heap.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86bd166f",
      "metadata": {},
      "source": [
        "### Copy vs View (ทำไม RAM พุ่ง)\n",
        "- pandas/numpy บางครั้งให้ **view** (ชี้ข้อมูลเดิม) → ประหยัด RAM\n",
        "- แต่บางครั้งสร้าง **copy** จริง → RAM ใช้เพิ่มทันที\n",
        "\n",
        "![Reference vs Copy](images/reference_vs_copy.png)\n",
        "- รู้ทันด้วย `.copy()` และสังเกต memory usage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b324fe31",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "arr = np.arange(10)\n",
        "view = arr[2:6]   # view\n",
        "view[0] = 999\n",
        "print('arr:', arr)  # เปลี่ยนตามเพราะแชร์ memory\n",
        "\n",
        "copy = arr[2:6].copy()\n",
        "copy[0] = 555\n",
        "print('arr after copy change:', arr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9368e32",
      "metadata": {},
      "source": [
        "### Garbage Collection (GC) & Memory Leak — สิ่งที่ Big Data Engineer ต้องรู้\n",
        "\n",
        "**1. Garbage Collection (GC)**:\n",
        "- คือ \"พนักงานทำความสะอาด\" อัตโนมัติ (ใน Java/Python) ที่คอยเดินเก็บกวาดหน่วยความจำ (RAM) ส่วนที่ไม่มีใครใช้แล้วคืนให้ระบบ\n",
        "- ถ้าไม่มี GC: โปรแกรมจะกิน RAM ไปเรื่อยๆ จนเครื่องค้าง\n",
        "\n",
        "**2. Memory Leak**:\n",
        "- คือการที่เรา \"เผลอผูกเชือก\" (Reference) ไว้กับขยะ ทำให้ GC ไม่กล้าเก็บไปทิ้ง เพราะนึกว่าเรายังใช้อยู่\n",
        "- **ผลกระทบใน Big Data**: Spark Driver/Executor มักจะตายด้วย error `OutOfMemory` เพราะ Code ของเราเขียน dataframe เก็บใส่ list ไว้เรื่อยๆ โดยไม่เคลียร์ทิ้ง\n",
        "\n",
        "![GC & Memory Leak](images/gc_memory_leak.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d18306f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: reference vs copy\n",
        "import sys, copy\n",
        "\n",
        "a = [1, 2, 3]\n",
        "b = a              # แชร์รีโมท\n",
        "c = copy.deepcopy(a)  # สำเนาของจริง\n",
        "\n",
        "print('id(a)', id(a), 'id(b)', id(b), 'id(c)', id(c))\n",
        "a.append(99)\n",
        "print('หลัง append ที่ a:')\n",
        "print('a =', a)\n",
        "print('b =', b, '<-- เปลี่ยนตามเพราะแชร์ reference')\n",
        "print('c =', c, '<-- ไม่เปลี่ยนเพราะ copy ของจริง')\n",
        "print('ขนาด list a (bytes):', sys.getsizeof(a))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f933c6da",
      "metadata": {},
      "source": [
        "### Intro to Pandas & DataFrame (ปูพื้นฐาน)\n",
        "- **Pandas**: คือ Library สามัญประจำบ้านของ Python สำหรับจัดการข้อมูลตาราง (เหมือน Excel ติดปีก)\n",
        "- **DataFrame**: คือ Object หลักของ Pandas หน้าตาเหมือนตาราง มี Row Index และ Column Name\n",
        "- แต่ระวัง! DataFrame โหลดข้อมูลทุกอย่างลง RAM (In-memory) ถ้าไฟล์ใหญ่กว่า RAM เครื่อง -> จบเห่ (ต้องใช้ Spark/Dask แทน)\n",
        "\n",
        "### Memory Footprint ของ DataFrame — เลือก dtype ให้เหมาะ\n",
        "- ค่า `int64/float64` หนักกว่า `int32/float32`\n",
        "- หมวดหมู่ซ้ำควรใช้ `category` → ลด RAM และทำงานกับ Parquet dictionary ได้ดี\n",
        "- ชอบตั้งคำถาม: \"คอลัมน์นี้จำเป็นต้องเป็นอะไรยาวสุดไหม?\" เช่น `user_id` 0-1e6 พอใช้ `int32`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66af50c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\n",
        "rows = 200_000\n",
        "raw_df = pd.DataFrame({\n",
        "    'user_id': np.random.randint(0, 1_000_000, rows),\n",
        "    'country': np.random.choice(['TH','US','JP','DE','FR','UK'], rows),\n",
        "    'value': np.random.randn(rows)\n",
        "})\n",
        "\n",
        "print('default dtypes:', raw_df.dtypes)\n",
        "print('memory (MB):', raw_df.memory_usage(deep=True).sum() / 1e6)\n",
        "\n",
        "optimized = raw_df.assign(\n",
        "    user_id = raw_df['user_id'].astype('int32'),\n",
        "    country = raw_df['country'].astype('category'),\n",
        "    value = raw_df['value'].astype('float32')\n",
        ")\n",
        "print('\\noptimized dtypes:', optimized.dtypes)\n",
        "print('memory (MB):', optimized.memory_usage(deep=True).sum() / 1e6)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d000b7",
      "metadata": {},
      "source": [
        "### Data Architecture Evolution: Warehouse vs Lake vs Lakehouse\n",
        "\n",
        "1. **Data Warehouse (ยุค 1990s)**: \n",
        "   - เก็บข้อมูลมีโครงสร้าง (Table) \n",
        "   - ข้อดี: เป็นระเบียบ, เร็ว, เชื่อถือได้ \n",
        "   - ข้อเสีย: แพง, ไม่รองรับไฟล์ภาพ/เสียง/Video\n",
        "\n",
        "2. **Data Lake (ยุค 2010s)**: \n",
        "   - ทะเลข้อมูล เททุกอย่างลงไป (Files) \n",
        "   - ข้อดี: ถูก, เก็บได้ทุกท่า \n",
        "   - ข้อเสีย: เละเทะ (Data Swamp), ไม่รองรับ Transaction (ACID), Update ยาก\n",
        "\n",
        "3. **Data Lakehouse (ยุค 2020s)**: \n",
        "   - ลูกผสม: เก็บเป็นไฟล์ใน Lake (ถูก) + มี Metadata Log คุม (ACID, Schema) \n",
        "   - ทำให้เราสามารถ `UPDATE/DELETE` ข้อมูลในไฟล์ Parquet ได้เหมือนทำใน Database!\n",
        "\n",
        "| คุณสมบัติ | Warehouse | Lake | Lakehouse (Delta/Iceberg/Hudi) |\n",
        "|---|---|---|---|\n",
        "| Schema enforcement | สูง | ต่ำ | ปานกลาง/สูง |\n",
        "| ACID | ✅ | ❌ | ✅ (ผ่าน log + manifest) |\n",
        "| Format หลัก | Proprietary | Any File | Open Format (Parquet) + Log |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7659d2a",
      "metadata": {},
      "source": [
        "## Part 3: Modern Storage Wars — Hands-on\n",
        "เปรียบเทียบ CSV (row-based) vs Parquet (columnar)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f608646c",
      "metadata": {},
      "source": [
        "### Format Zoo — CSV/JSON/Avro/Parquet/ORC\n",
        "| Format | โครงสร้าง | จุดเด่น | จุดอ่อน |\n",
        "|---|---|---|---|\n",
        "| CSV | Row | อ่านง่าย, ใช้ได้ทุกที่ | ไม่มี schema, ใหญ่, ช้า |\n",
        "| JSON | Row/Nested | ยืดหยุ่น, nested ได้ | ใหญ่, parsing ช้า |\n",
        "| Avro | Row + Schema | schema ชัด, write fast | ไม่เหมาะสำหรับ column pruning |\n",
        "| Parquet | Columnar | อ่านเร็ว, compression ดี | write ช้ากว่า row format |\n",
        "| ORC | Columnar | สถิติละเอียด, compact | tooling น้อยกว่า Parquet |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8bb7717",
      "metadata": {},
      "source": [
        "### Row vs Columnar — เลือกเก็บอย่างไร\n",
        "- **Row (CSV/JSON)**: เขียน/อ่านทั้งแถวรวดเดียว → เหมาะกับ workload เขียนต่อเนื่องหรืออ่านทุกคอลัมน์\n",
        "- **Columnar (Parquet/ORC)**: จัดเก็บทีละคอลัมน์ → อ่านเฉพาะคอลัมน์ที่ใช้ (column pruning), บีบอัดได้สูง, รองรับ predicate pushdown\n",
        "- จุดตัดสินใจ: ถ้า query ส่วนใหญ่เลือกไม่กี่คอลัมน์หรือมี filter ชัดเจน → columnar ชนะ; ถ้าต้อง append record แบบ real-time และอ่านทุกคอลัมน์ → row-format อาจพอเพียง\n",
        "\n",
        "![Row vs Columnar](images/row_vs_columnar.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a14ee482",
      "metadata": {},
      "source": [
        "### ตัวอย่าง Column Pruning แบบภาพจำ\n",
        "มีตาราง 5 คอลัมน์ แต่ query ใช้แค่ 2 คอลัมน์: columnar จะอ่านเฉพาะ 2 คอลัมน์นั้น\n",
        "→ ประหยัด I/O และเวลามาก โดยเฉพาะเมื่อตารางมีหลายคอลัมน์\n",
        "\n",
        "![Column Pruning](images/column_pruning.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf66ff66",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, time, duckdb\n",
        "np.random.seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611ba3dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# สร้างข้อมูลจำลอง\n",
        "\n",
        "def generate_data(rows=1_000_000):\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(rows),\n",
        "        'category': np.random.choice(['Electronic', 'Clothing', 'Furniture', 'Food'], rows),\n",
        "        'status': np.random.choice(['Completed', 'Pending', 'Failed'], rows),\n",
        "        'value': np.random.randn(rows),\n",
        "        'description': np.random.choice(['Data for big data class'] * 5, rows)\n",
        "    })\n",
        "    return df\n",
        "\n",
        "%time df = generate_data()\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4126f25a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save & เปรียบเทียบขนาดไฟล์\n",
        "%time df.to_csv('data.csv', index=False)\n",
        "%time df.to_parquet('data.parquet', index=False)\n",
        "\n",
        "csv_mb = os.path.getsize('data.csv') / (1024 * 1024)\n",
        "parquet_mb = os.path.getsize('data.parquet') / (1024 * 1024)\n",
        "\n",
        "print(f\"CSV Size: {csv_mb:.2f} MB\")\n",
        "print(f\"Parquet Size: {parquet_mb:.2f} MB\")\n",
        "print(f\"เล็กลงกี่เท่า: {csv_mb / parquet_mb:.1f}x\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee41dc5a",
      "metadata": {},
      "source": [
        "### Read Only Needed Columns (pyarrow)\n",
        "อ่านเฉพาะคอลัมน์ที่ใช้จริง → เร็วขึ้น และใช้ RAM น้อยลง\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089f4ddc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow.parquet as pq\n",
        "pf = pq.ParquetFile('data.parquet')\n",
        "# อ่านเฉพาะคอลัมน์ที่ต้องใช้\n",
        "subset = pf.read(columns=['status', 'value']).to_pandas()\n",
        "subset.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b6484c",
      "metadata": {},
      "source": [
        "### Row Group Size Experiment\n",
        "ลองเขียน Parquet ด้วย row_group_size ต่างกัน แล้วดูผลต่อขนาด/เวลาอ่าน\n",
        "แนวคิด: row group ใหญ่เกินไป → pruning น้อย, เล็กเกินไป → metadata เยอะ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e02b98",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow as pa, pyarrow.parquet as pq, os, time\n",
        "\n",
        "table = pa.Table.from_pandas(df)\n",
        "for rg in [50_000, 200_000]:\n",
        "    fn = f\"rg_{rg}.parquet\"\n",
        "    pq.write_table(table, fn, row_group_size=rg)\n",
        "    print(fn, os.path.getsize(fn)/(1024*1024), 'MB')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28e0a3d5",
      "metadata": {},
      "source": [
        "### Compression Shootout (Snappy vs Gzip vs Zstd)\n",
        "ลองบีบอัด Parquet ด้วย codec ต่าง ๆ แล้วดูขนาด/เวลาเขียน\n",
        "- Snappy: เร็ว, ขนาดพอประมาณ (เหมาะ default)\n",
        "- Gzip: ช้ากว่า แต่เล็กลงอีก\n",
        "- Zstd: สมดุล ขนาดเล็กกว่า Snappy แต่เร็วกว่า Gzip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a369da",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time, os\n",
        "import pandas as pd\n",
        "\n",
        "codecs = ['snappy','gzip','zstd','none']\n",
        "rows = 400_000\n",
        "cdf = df.sample(rows, replace=False).reset_index(drop=True)\n",
        "\n",
        "results = []\n",
        "for codec in codecs:\n",
        "    fn = f\"data_{codec}.parquet\"\n",
        "    start = time.time()\n",
        "    cdf.to_parquet(fn, compression=None if codec=='none' else codec, index=False)\n",
        "    elapsed = time.time() - start\n",
        "    size_mb = os.path.getsize(fn)/(1024*1024)\n",
        "    results.append((codec, size_mb, elapsed))\n",
        "\n",
        "pd.DataFrame(results, columns=['codec','size_mb','write_sec']).sort_values('size_mb')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2b1d43",
      "metadata": {},
      "source": [
        "### Parquet Internals แบบสั้นๆ\n",
        "- ไฟล์ถูกแบ่งเป็น **Row Groups** (มัก ~128MB) → เป็นหน่วยที่ถูกอ่าน/ข้ามได้\n",
        "- แต่ละคอลัมน์ใน row group มี **Dictionary** (ถ้าเปิด) + **min/max stats** + optional **Bloom filter**\n",
        "- เวลา query: engine ดู metadata ก่อน → ตัด row group ที่ค่า min/max ไม่ทับกับเงื่อนไข (row-group pruning) แล้วค่อยอ่านเฉพาะคอลัมน์ที่ใช้\n",
        "- Stats ยิ่งแม่นเมื่อข้อมูลถูก sort/cluster ตามคอลัมน์ที่เรามัก filter\n",
        "\n",
        "![Parquet Structure](images/parquet_structure.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87224f2f",
      "metadata": {},
      "source": [
        "### ทำไม Parquet ถึงเล็กและอ่านเร็ว?\n",
        "- **Dictionary Encoding**: เก็บตาราง mapping แล้วเก็บเป็นตัวเลขสั้น ๆ\n",
        "- **Run-Length Encoding (RLE)**: ค่าซ้ำกันยาว ๆ เก็บเป็น “ค่า × จำนวนครั้ง”\n",
        "- **Columnar Layout**: เก็บทีละคอลัมน์ → อ่านเฉพาะคอลัมน์ที่ต้องใช้ (column pruning)\n",
        "- **Predicate Pushdown**: อ่านเฉพาะ row group ที่เงื่อนไขตรง (เช่น status = 'Completed')\n",
        "\n",
        "![Encoding Techniques](images/encoding_techniques.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b42fe60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: column pruning & predicate pushdown ด้วย DuckDB\n",
        "# DuckDB จะอ่านไฟล์โดยตรง (ไม่ต้อง load ทั้งไฟล์เข้าหน่วยความจำก่อน)\n",
        "\n",
        "# อ่านเฉพาะ 2 คอลัมน์\n",
        "%time duckdb.query(\"SELECT category, status FROM 'data.csv' LIMIT 5\").df()\n",
        "%time duckdb.query(\"SELECT category, status FROM 'data.parquet' LIMIT 5\").df()\n",
        "\n",
        "# นับจำนวนสถานะ Completed (predicate pushdown ทำงานกับ Parquet)\n",
        "%time duckdb.query(\"SELECT count(*) FROM 'data.csv' WHERE status='Completed'\").df()\n",
        "%time duckdb.query(\"SELECT count(*) FROM 'data.parquet' WHERE status='Completed'\").df()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f117963",
      "metadata": {},
      "source": [
        "### Read Speed Compare — DuckDB vs pandas\n",
        "- DuckDB อ่านจากไฟล์โดยตรง ใช้ predicate/column pruning เต็มที่\n",
        "- pandas ต้องโหลดทั้งไฟล์ (แต่กับ Parquet ยังได้ column pruning บางส่วน)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94ccaa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, time\n",
        "\n",
        "start = time.time(); pd.read_csv('data.csv', usecols=['status']).query(\"status=='Completed'\").shape[0]; csv_time = time.time()-start\n",
        "start = time.time(); pd.read_parquet('data.parquet', columns=['status']).query(\"status=='Completed'\").shape[0]; pq_time = time.time()-start\n",
        "\n",
        "print(f\"pandas CSV count took {csv_time:.2f}s; Parquet took {pq_time:.2f}s\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639f199e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect Parquet metadata (row groups + stats) ด้วย PyArrow\n",
        "import pyarrow.parquet as pq\n",
        "pf = pq.ParquetFile('data.parquet')\n",
        "print('Row groups:', pf.num_row_groups)\n",
        "print('Schema:')\n",
        "print(pf.schema)\n",
        "\n",
        "# ดูสถิติของคอลัมน์ status ใน row group แรก\n",
        "rg0 = pf.metadata.row_group(0).column(pf.schema.names.index('status'))\n",
        "print('status stats rowgroup0:', rg0.statistics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d8e9e6",
      "metadata": {},
      "source": [
        "### Data Quality Check Mini-lab\n",
        "- ตรวจ missing values / duplicate IDs\n",
        "- คำนวณ % missing แล้วตัดสินใจว่าจะลบ เติม หรือเก็บไว้\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e5cc2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ตรวจข้อมูลคุณภาพเบื้องต้น\n",
        "print('missing per column:')\n",
        "print(df.isna().mean())\n",
        "print('duplicate id count:', df['id'].duplicated().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd71c88",
      "metadata": {},
      "source": [
        "### Activity 2 — Cardinality & Compression\n",
        "ลองปรับจำนวน category/status ให้หลากหลายขึ้น แล้ววัดขนาด Parquet เทียบ CSV\n",
        "- คาดหวัง: เมื่อค่าซ้ำลดลง (cardinality สูง) ประสิทธิภาพการบีบอัดจะลดลง\n",
        "- ให้ลอง rows = 200_000, 1_000_000 และ cardinality ต่าง ๆ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50856fc2",
      "metadata": {},
      "source": [
        "#### Partitioning: ทำแบบไหนดี?\n",
        "- ดี: `year/month/day` สำหรับข้อมูลตามเวลา → filter ง่าย\n",
        "- ระวัง: partition ตาม `user_id` หรือ `country` ที่มีค่ากระจายมาก → ไฟล์เล็กนับหมื่น\n",
        "- ใช้ `repartition` เพื่อคุมจำนวนไฟล์ เป้าหมายขนาด 128–512MB/ไฟล์\n",
        "- หลัง append หลายรอบควรมีขั้นตอน **compaction** รวมไฟล์\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfbebf76",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment: cardinality vs size (Parquet vs CSV)\n",
        "import pandas as pd, numpy as np, os, time\n",
        "\n",
        "rows = 300_000\n",
        "cardinalities = [4, 20, 200, 1000]\n",
        "results = []\n",
        "\n",
        "for card in cardinalities:\n",
        "    categories = [f\"cat_{i}\" for i in range(card)]\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(rows),\n",
        "        'cat': np.random.choice(categories, rows),\n",
        "        'value': np.random.randn(rows)\n",
        "    })\n",
        "    csv_path = f\"card_{card}.csv\"\n",
        "    pq_path = f\"card_{card}.parquet\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_parquet(pq_path, index=False)\n",
        "    csv_mb = os.path.getsize(csv_path)/(1024*1024)\n",
        "    pq_mb = os.path.getsize(pq_path)/(1024*1024)\n",
        "    results.append((card, csv_mb, pq_mb, csv_mb/pq_mb))\n",
        "\n",
        "pd.DataFrame(results, columns=['cardinality','csv_mb','parquet_mb','ratio_csv_div_parquet']).sort_values('cardinality')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133bdd80",
      "metadata": {},
      "source": [
        "### Schema Evolution Mini-lab\n",
        "- เพิ่มคอลัมน์ใหม่ (เช่น `discount`) แล้วเขียนทับในโฟลเดอร์ Parquet เดิม\n",
        "- สังเกตว่าไฟล์ใหม่มี schema ต่างจากไฟล์เก่า → query engine ต้อง union schema\n",
        "- DuckDB / Spark รองรับการ union schema อัตโนมัติถ้าตั้งค่าเปิด\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3338b70d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# เพิ่มคอลัมน์ใหม่และเขียนทับบาง partition\n",
        "import pandas as pd\n",
        "new_df = df.copy()\n",
        "new_df['discount'] = np.random.randint(0, 30, len(new_df))\n",
        "\n",
        "duckdb.query(\"COPY (SELECT * FROM new_df) TO 'lakehouse' (FORMAT 'parquet', PARTITION_BY ['cat'])\")\n",
        "\n",
        "# อ่านรวม schema\n",
        "duckdb.query(\"SELECT count(*), avg(discount) FROM 'lakehouse'\").df().head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02575bb",
      "metadata": {},
      "source": [
        "### Quick Check (ระหว่างคาบ)\n",
        "1) ทำไม NameNode/S3 list หลายพันไฟล์ถึงช้า แม้ไฟล์จะเล็กมาก?  \n",
        "2) ถ้า query ส่วนใหญ่เลือกแค่ 3 คอลัมน์จาก 200 คอลัมน์ คุณจะเลือกฟอร์แมตอะไร? ทำไม?  \n",
        "3) จะตั้ง partition อย่างไรสำหรับตาราง log ต่อวันให้ query `WHERE date BETWEEN ...` เร็ว แต่ไม่ระเบิดจำนวนไฟล์?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22fdeddf",
      "metadata": {},
      "source": [
        "### Activity 3 — Mini Lakehouse (Partition & Query)\n",
        "1. สร้างโฟลเดอร์ `lakehouse/` และเขียน Parquet แบบ partition ตาม `status`\n",
        "2. ใช้ DuckDB อ่านจากโฟลเดอร์นั้น เช่น `SELECT status, COUNT(*) FROM 'lakehouse' GROUP BY 1`\n",
        "3. ลอง query เฉพาะ `status='Failed'` เพื่อดูเวลา (partition pruning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b4f12f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build mini lakehouse partitioned by status\n",
        "import os, duckdb\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "os.makedirs('lakehouse', exist_ok=True)\n",
        "# เขียนแบบ partition\n",
        "duckdb.query(\"COPY (SELECT * FROM df) TO 'lakehouse' (FORMAT 'parquet', PARTITION_BY ['status'])\")\n",
        "\n",
        "# Query ทั้งก้อน\n",
        "%time duckdb.query(\"SELECT status, COUNT(*) AS cnt, AVG(value) AS avg_value FROM 'lakehouse' GROUP BY 1\").df()\n",
        "\n",
        "# Query เฉพาะ Failed (จะ prune partition อื่น)\n",
        "%time duckdb.query(\"SELECT * FROM 'lakehouse' WHERE status='Failed' LIMIT 5\").df()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56d7432",
      "metadata": {},
      "source": [
        "### Lab Report Template (ส่งท้ายคาบ)\n",
        "ให้สรุปผลการทดลองเป็น 1 หน้าสั้น ๆ:\n",
        "- ขนาดไฟล์ CSV vs Parquet (กี่เท่า?)\n",
        "- เวลาอ่านคอลัมน์เดียว vs ทั้งตาราง\n",
        "- วิธี partition ที่เลือกและเหตุผล\n",
        "- ข้อสังเกตเรื่อง small files หรือ row group size\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbae6fc5",
      "metadata": {},
      "source": [
        "## Wrap-up & Post-test\n",
        "**Takeaways**\n",
        "1. File system มี block/fragmentation → small files problem\n",
        "2. DB ชนะไฟล์ตรงที่ ACID + Index\n",
        "3. Stack = รีโมท, Heap = ทีวี, GC เก็บของที่ไม่มีรีโมทชี้\n",
        "4. Parquet/Columnar = เล็กและเร็วด้วย dictionary + RLE + column pruning + predicate pushdown\n",
        "5. Lakehouse = Parquet + Transaction Log (Delta/Iceberg/Hudi)\n",
        "\n",
        "**Post-test**\n",
        "1) อธิบาย Stack vs Heap ด้วยอุปมาใดก็ได้  \n",
        "2) ACID สำคัญอย่างไรกับข้อมูลการเงิน?  \n",
        "3) Dictionary Encoding ลดขนาดไฟล์ได้อย่างไร?  \n",
        "4) Data Lakehouse ต่างจาก Data Warehouse อย่างไร?  \n",
        "5) Column pruning / predicate pushdown ช่วยประหยัดเวลาอ่านไฟล์ได้อย่างไร?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7199c29f",
      "metadata": {},
      "source": [
        "### After Class — อ่านต่อถ้ามีเวลา\n",
        "- DuckDB Docs: Query Parquet & metadata, filter/column pushdown\n",
        "- AWS EMR/Athena Best Practices: ขนาดไฟล์ 128–512MB, หลีกเลี่ยง partition ลึก\n",
        "- Blog: Parquet predicate pushdown (row-group pruning, dictionary, bloom filter)\n",
        "- HDFS Small Files Problem: ทำไม metadata จึงล่มง่าย และวิธีแก้ด้วย compaction\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "title": "Big Data Week 2"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}